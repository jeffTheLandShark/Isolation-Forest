{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e354ea9f",
   "metadata": {},
   "source": [
    "# Term Project - Isolation Forest for Anomaly Detection\n",
    "> Leigh Goetsch </br>\n",
    "> CSC 5601 - Theory of Machine Learning </br>\n",
    "> Milwaukee School of Engineering </br>\n",
    "> Fall 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9c5944",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87792a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from isolation_forest import IsolationForest\n",
    "import pandas as pd\n",
    "from scipy import io as sio\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d0b93d",
   "metadata": {},
   "source": [
    "## Vertebral Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b37959",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../Data/vertebral.mat\"\n",
    "# data_path = \"../Data/satellite.mat\"\n",
    "# data_path = \"../Data/satimage-2.mat\"\n",
    "mat = sio.loadmat(data_path)\n",
    "\n",
    "X = mat[\"X\"]\n",
    "y = mat[\"y\"].flatten()\n",
    "feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "df_vertebral = pd.DataFrame(X, columns=feature_names)\n",
    "df_vertebral[\"target\"] = y\n",
    "\n",
    "df_vertebral[\"target\"].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eba8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iso_forest = IsolationForest(random_state=42, contamination=0.205)\n",
    "predictions = iso_forest.fit_predict(X)\n",
    "classification_report(y, predictions, target_names=[\"Inlier\", \"Outlier\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scatter grid of all feature pairs\n",
    "pd.plotting.scatter_matrix(\n",
    "    df_vertebral[feature_names],\n",
    "    c=y,\n",
    "    figsize=(10, 10),\n",
    "    marker=\"o\",\n",
    "    hist_kwds={\"bins\": 20},\n",
    "    s=60,\n",
    "    alpha=0.8,\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "pd.plotting.scatter_matrix(\n",
    "    df_vertebral[feature_names],\n",
    "    c=predictions,\n",
    "    figsize=(10, 10),\n",
    "    marker=\"o\",\n",
    "    hist_kwds={\"bins\": 20},\n",
    "    s=60,\n",
    "    alpha=0.8,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192163b9",
   "metadata": {},
   "source": [
    "## Malware Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841093ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"../Data/IRIS.csv\"\n",
    "data_path = \"../Data/TUANDROMD.csv\"\n",
    "df_data = pd.read_csv(data_path)\n",
    "\n",
    "# Drop columns with identical values (constant columns)\n",
    "nunique = df_data.nunique()\n",
    "df_data = df_data.drop(columns=nunique[nunique == 1].index)\n",
    "\n",
    "# Drop rows with missing values\n",
    "df_data = df_data.dropna()\n",
    "\n",
    "# Encode target: anomalies = 1, inliers = 0\n",
    "df_data[\"target\"] = np.where(df_data[\"target\"] == 1, 0, 1)\n",
    "\n",
    "X = df_data.drop(columns=[\"target\"]).values\n",
    "y = df_data[\"target\"].values\n",
    "\n",
    "contamination = df_data[\"target\"].value_counts(normalize=True)[1]\n",
    "\n",
    "X.shape, y.shape, df_data.shape, df_data[\"target\"].value_counts(), contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4044b097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline model with default parameters\n",
    "baseline_iso_forest = IsolationForest(random_state=42, contamination=contamination)\n",
    "baseline_predictions = baseline_iso_forest.fit_predict(X)\n",
    "\n",
    "# Store baseline metrics\n",
    "baseline_metrics = {\n",
    "    \"precision\": precision_score(y, baseline_predictions),\n",
    "    \"recall\": recall_score(y, baseline_predictions),\n",
    "    \"f1\": f1_score(y, baseline_predictions),\n",
    "}\n",
    "\n",
    "print(\n",
    "    classification_report(y, baseline_predictions, target_names=[\"Inlier\", \"Outlier\"])\n",
    ")\n",
    "print(f'\\nBaseline F1 Score: {baseline_metrics[\"f1\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ec749",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = df_data.columns.drop(\"target\")\n",
    "importances = baseline_iso_forest.feature_importances_()\n",
    "importance_lookup = dict(zip(feature_names, importances))\n",
    "importance_df = (\n",
    "    pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
    "    .sort_values(by=\"Importance\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for feature in feature_names:\n",
    "    # group once per feature\n",
    "    grp = df_data.groupby([feature, \"target\"]).size().unstack(\"target\", fill_value=0)\n",
    "    grp = grp.rename(columns={0: \"inliers\", 1: \"outliers\"})\n",
    "\n",
    "    # total counts for that feature\n",
    "    total = df_data[feature].value_counts().rename(\"total_count\")\n",
    "\n",
    "    # merge and tidy\n",
    "    merged = grp.merge(total, left_index=True, right_index=True)\n",
    "    merged[\"feature\"] = feature\n",
    "    merged[\"importance\"] = importance_lookup[feature]\n",
    "    merged.reset_index(inplace=True)\n",
    "    merged.rename(columns={feature: \"value\"}, inplace=True)\n",
    "\n",
    "    rows.append(merged)\n",
    "\n",
    "info_df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "# weight importance by outlier ratio\n",
    "info_df[\"fraction_outliers\"] = info_df[\"outliers\"] / info_df[\"total_count\"]\n",
    "info_df.sort_values(by=[\"importance\"], ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2fa7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    data=info_df,\n",
    "    x=\"importance\",\n",
    "    y=\"fraction_outliers\",\n",
    "    size=\"total_count\",\n",
    "    hue=\"value\",\n",
    "    alpha=0.5,\n",
    ")\n",
    "plt.title(\"Feature Importance vs Outlier Ratio\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Outlier Ratio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2fa7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratively remove least important features and evaluate performance\n",
    "selection_results = []\n",
    "current_features = list(feature_names.values)\n",
    "least_important = \"\"\n",
    "\n",
    "\n",
    "def get_least_important(importances, features, i) -> str:\n",
    "    # Get feature importances\n",
    "    importance_dict = dict(zip(current_features, importances))\n",
    "\n",
    "    # Remove i least important feature\n",
    "    # # print importance dict sorted by value\n",
    "    # print(\"Current feature importances:\")\n",
    "    # for feat, imp in sorted(importance_dict.items(), key=lambda item: item[1]):\n",
    "    #     print(f\"{feat}: {imp:.6f}\")\n",
    "    return sorted(importance_dict, key=importance_dict.get)[i]\n",
    "\n",
    "\n",
    "def test_model(current_X) -> dict:\n",
    "    iso_forest = IsolationForest(\n",
    "        n_trees=500, random_state=42, contamination=contamination\n",
    "    )\n",
    "    predictions = iso_forest.fit_predict(current_X)\n",
    "    # weight the model metrics by contamination\n",
    "\n",
    "    # Calculate metrics\n",
    "    return {\n",
    "        \"f1\": f1_score(y, predictions, average=\"weighted\"),\n",
    "        \"precision\": precision_score(y, predictions, average=\"weighted\"),\n",
    "        \"recall\": recall_score(y, predictions, average=\"weighted\"),\n",
    "        \"iforest\": iso_forest,\n",
    "    }\n",
    "\n",
    "f1 = 0\n",
    "least_important = \"\"\n",
    "importances = np.zeros(len(current_features))\n",
    "while len(current_features) > 15:\n",
    "    i = 0\n",
    "    last_f1 = f1\n",
    "    f1 = -1\n",
    "    precision, recall = 0, 0\n",
    "    feature = least_important\n",
    "    best_feature = feature\n",
    "    while last_f1 > f1 and i < len(current_features) // 10:\n",
    "        tmp_features = current_features.copy()\n",
    "        if importances.sum() > 0:\n",
    "            feature = get_least_important(importances, current_features, i)\n",
    "            tmp_features.remove(feature)\n",
    "        # Get current feature subset\n",
    "        current_X = df_data[tmp_features].values\n",
    "        results = test_model(current_X)\n",
    "        if results[\"f1\"] > f1:\n",
    "            best_feature = feature\n",
    "            precision = results[\"precision\"]\n",
    "            recall = results[\"recall\"]\n",
    "            f1 = results[\"f1\"]\n",
    "        importances = results[\"iforest\"].feature_importances_()\n",
    "        i += 1\n",
    "    least_important = best_feature\n",
    "    if feature != \"\":\n",
    "        current_features.remove(least_important)\n",
    "\n",
    "    # Train model with current features\n",
    "\n",
    "    selection_results.append(\n",
    "        {\n",
    "            \"num_features\": len(current_features),\n",
    "            \"features\": current_features.copy(),\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Features: {len(current_features):3d} | F1: {f1:.4f} | Removed: {least_important}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f959fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature selection results\n",
    "results_df = pd.DataFrame(selection_results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].plot(results_df[\"num_features\"], results_df[\"f1\"], marker=\"o\")\n",
    "axes[0].set_xlabel(\"Number of Features\")\n",
    "axes[0].set_ylabel(\"F1 Score\")\n",
    "axes[0].set_title(\"F1 Score vs Number of Features\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "axes[1].plot(\n",
    "    results_df[\"num_features\"], results_df[\"precision\"], marker=\"o\", label=\"Precision\"\n",
    ")\n",
    "axes[1].plot(\n",
    "    results_df[\"num_features\"], results_df[\"recall\"], marker=\"s\", label=\"Recall\"\n",
    ")\n",
    "axes[1].set_xlabel(\"Number of Features\")\n",
    "axes[1].set_ylabel(\"Score\")\n",
    "axes[1].set_title(\"Precision & Recall vs Number of Features\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "axes[2].bar(range(len(results_df)), results_df[\"f1\"])\n",
    "axes[2].set_xlabel(\"Feature Count\")\n",
    "axes[2].set_ylabel(\"F1 Score\")\n",
    "axes[2].set_title(\"F1 Score for Each Feature Count\")\n",
    "axes[2].set_xticks(range(len(results_df)))\n",
    "axes[2].set_xticklabels(results_df[\"num_features\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best feature subset\n",
    "best_idx = results_df[\"f1\"].idxmax()\n",
    "best_features = selection_results[best_idx][\"features\"]\n",
    "best_f1 = selection_results[best_idx][\"f1\"]\n",
    "\n",
    "print(f\"\\n=== BEST FEATURE SUBSET ===\")\n",
    "print(f\"Number of features: {len(best_features)}\")\n",
    "print(f\"Selected features: {best_features}\")\n",
    "print(f\"F1 Score: {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb80760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data with best features\n",
    "X_best = df_data[best_features].values\n",
    "\n",
    "# Define parameter grid for tuning (using custom IsolationForest parameters)\n",
    "param_grid = {\n",
    "    \"n_trees\": [100, 150, 200, 500, 1000],\n",
    "    \"max_samples\": [128, 256, 512, 1024],\n",
    "    \"contamination\": [0.15, 0.2, 0.25],\n",
    "}\n",
    "\n",
    "print(\"Parameter Grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
    "print(f\"\\nTotal parameter combinations: {total_combinations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a66a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search with cross-validation\n",
    "best_params = None\n",
    "best_score = -1\n",
    "grid_results = []\n",
    "\n",
    "n_trees_list = param_grid[\"n_trees\"]\n",
    "max_samples_list = param_grid[\"max_samples\"]\n",
    "contamination_list = param_grid[\"contamination\"]\n",
    "\n",
    "total_evals = 0\n",
    "total_combinations = len(n_trees_list) * len(max_samples_list) * len(contamination_list)\n",
    "\n",
    "for n_tr in n_trees_list:\n",
    "    for max_samp in max_samples_list:\n",
    "        for contam in contamination_list:\n",
    "            total_evals += 1\n",
    "\n",
    "            # Use cross-validation\n",
    "            cv_scores = []\n",
    "            skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "            for train_idx, val_idx in skf.split(X_best, y):\n",
    "                X_train, X_val = X_best[train_idx], X_best[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "                # Train model with custom IsolationForest parameters\n",
    "                iso_forest = IsolationForest(\n",
    "                    n_trees=n_tr,\n",
    "                    max_samples=max_samp,\n",
    "                    contamination=contam,\n",
    "                    random_state=42,\n",
    "                )\n",
    "                iso_forest.fit(X_train)\n",
    "                pred = iso_forest.predict(X_val)\n",
    "\n",
    "                # Score\n",
    "                score = f1_score(y_val, pred)\n",
    "                cv_scores.append(score)\n",
    "\n",
    "            mean_cv_score = np.mean(cv_scores)\n",
    "\n",
    "            grid_results.append(\n",
    "                {\n",
    "                    \"n_trees\": n_tr,\n",
    "                    \"max_samples\": max_samp,\n",
    "                    \"contamination\": contam,\n",
    "                    \"mean_cv_f1\": mean_cv_score,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if mean_cv_score > best_score:\n",
    "                best_score = mean_cv_score\n",
    "                best_params = {\n",
    "                    \"n_trees\": n_tr,\n",
    "                    \"max_samples\": max_samp,\n",
    "                    \"contamination\": contam,\n",
    "                }\n",
    "\n",
    "            if total_evals % 5 == 0:\n",
    "                print(\n",
    "                    f\"[{total_evals}/{total_combinations}] Best CV F1: {best_score:.4f}\"\n",
    "                )\n",
    "\n",
    "print(f\"\\nCompleted {total_evals} parameter evaluations\")\n",
    "print(f\"\\n=== BEST PARAMETERS ===\")\n",
    "for param, value in best_params.items():\n",
    "    print(f\"{param}: {value}\")\n",
    "print(f\"Best CV F1 Score: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653e4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze grid search results\n",
    "grid_results_df = pd.DataFrame(grid_results).sort_values(\"mean_cv_f1\", ascending=False)\n",
    "\n",
    "print(\"\\n=== TOP 10 PARAMETER COMBINATIONS ===\")\n",
    "print(grid_results_df.head(10))\n",
    "\n",
    "# Analyze parameter importance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# n_trees impact\n",
    "n_tr_impact = grid_results_df.groupby(\"n_trees\")[\"mean_cv_f1\"].agg([\"mean\", \"std\"])\n",
    "axes[0].errorbar(\n",
    "    n_tr_impact.index, n_tr_impact[\"mean\"], yerr=n_tr_impact[\"std\"], marker=\"o\"\n",
    ")\n",
    "axes[0].set_xlabel(\"n_trees\")\n",
    "axes[0].set_ylabel(\"Mean CV F1\")\n",
    "axes[0].set_title(\"Impact of n_trees\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# contamination impact\n",
    "contam_impact = grid_results_df.groupby(\"contamination\")[\"mean_cv_f1\"].agg(\n",
    "    [\"mean\", \"std\"]\n",
    ")\n",
    "axes[1].errorbar(\n",
    "    contam_impact.index, contam_impact[\"mean\"], yerr=contam_impact[\"std\"], marker=\"o\"\n",
    ")\n",
    "axes[1].set_xlabel(\"contamination\")\n",
    "axes[1].set_ylabel(\"Mean CV F1\")\n",
    "axes[1].set_title(\"Impact of contamination\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "# max_samples impact\n",
    "max_samp_impact = grid_results_df.groupby(\"max_samples\")[\"mean_cv_f1\"].agg(\n",
    "    [\"mean\", \"std\"]\n",
    ")\n",
    "axes[2].errorbar(\n",
    "    max_samp_impact.index,\n",
    "    max_samp_impact[\"mean\"],\n",
    "    yerr=max_samp_impact[\"std\"],\n",
    "    marker=\"o\",\n",
    ")\n",
    "axes[2].set_xlabel(\"max_samples\")\n",
    "axes[2].set_ylabel(\"Mean CV F1\")\n",
    "axes[2].set_title(\"Impact of max_samples\")\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best features and best parameters\n",
    "final_iso_forest = IsolationForest(\n",
    "    n_trees=best_params[\"n_trees\"],\n",
    "    max_samples=best_params[\"max_samples\"],\n",
    "    contamination=best_params[\"contamination\"],\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "final_predictions = final_iso_forest.fit_predict(X_best)\n",
    "\n",
    "# Calculate final metrics\n",
    "final_metrics = {\n",
    "    \"precision\": precision_score(y, final_predictions),\n",
    "    \"recall\": recall_score(y, final_predictions),\n",
    "    \"f1\": f1_score(y, final_predictions),\n",
    "}\n",
    "\n",
    "print(\"\\n=== FINAL MODEL METRICS ===\")\n",
    "print(classification_report(y, final_predictions, target_names=[\"Inlier\", \"Outlier\"]))\n",
    "print(f'\\nFinal F1 Score: {final_metrics[\"f1\"]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6ca9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline vs final model\n",
    "comparison_df = pd.DataFrame(\n",
    "    {\"Baseline\": baseline_metrics, \"Final Model\": final_metrics}\n",
    ")\n",
    "\n",
    "print(\"\\n=== BASELINE VS FINAL MODEL COMPARISON ===\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Calculate improvements\n",
    "improvements = {}\n",
    "for metric in baseline_metrics.keys():\n",
    "    improvement = final_metrics[metric] - baseline_metrics[metric]\n",
    "    pct_change = (\n",
    "        (improvement / baseline_metrics[metric]) * 100\n",
    "        if baseline_metrics[metric] != 0\n",
    "        else 0\n",
    "    )\n",
    "    improvements[metric] = {\"absolute\": improvement, \"percent\": pct_change}\n",
    "\n",
    "print(\"\\n=== IMPROVEMENTS ===\")\n",
    "for metric, change in improvements.items():\n",
    "    print(f'{metric.upper()}: {change[\"absolute\"]:+.4f} ({change[\"percent\"]:+.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2603cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrices\n",
    "cm_baseline = confusion_matrix(y, baseline_predictions)\n",
    "cm_final = confusion_matrix(y, final_predictions)\n",
    "\n",
    "sns.heatmap(cm_baseline, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[0], cbar=False)\n",
    "axes[0].set_title(\"Baseline Model - Confusion Matrix\")\n",
    "axes[0].set_ylabel(\"True Label\")\n",
    "axes[0].set_xlabel(\"Predicted Label\")\n",
    "\n",
    "sns.heatmap(cm_final, annot=True, fmt=\"d\", cmap=\"Greens\", ax=axes[1], cbar=False)\n",
    "axes[1].set_title(\"Final Model - Confusion Matrix\")\n",
    "axes[1].set_ylabel(\"True Label\")\n",
    "axes[1].set_xlabel(\"Predicted Label\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Metrics comparison bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "metrics = list(baseline_metrics.keys())\n",
    "baseline_vals = [baseline_metrics[m] for m in metrics]\n",
    "final_vals = [final_metrics[m] for m in metrics]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width / 2, baseline_vals, width, label=\"Baseline\", alpha=0.8)\n",
    "ax.bar(x + width / 2, final_vals, width, label=\"Final Model\", alpha=0.8)\n",
    "\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Baseline vs Final Model Performance\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([m.capitalize() for m in metrics])\n",
    "ax.legend()\n",
    "ax.set_ylim([0, 1])\n",
    "ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csc5601",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
